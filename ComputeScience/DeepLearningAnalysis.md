# 딥러닝 분석
## 딥러닝
### 딥러닝
- ![image](https://github.com/googoo9918/TIL/assets/102513932/ec382bb6-d69c-4687-af56-b5180d566c26)
  - 딥러닝과 머신러닝
    - 공통점
      - 손실함수를 최소화
    - 차이점
      - 딥러닝은 머신러닝에 비해 인간의 인지능력에 강점
      - 은닉층 존재 여부
- ![image](https://github.com/googoo9918/TIL/assets/102513932/3ff859fb-0629-4ca2-8afd-b4875a92f1d4)
  - 독립변수(feature; x)들의 선형 및 비선형 결합을 통해 목적변수(종속변수, output; y)를 확률적으로 추정
  - 딥러닝은 은닉층 존재, 머신러닝은 은닉층 없음
### 은닉층
- *특성변수*는 회귀와 분류를 위한 *목적변수*를 예측하는데 이용
  - 특성변수는 *고수준의 대표성*을 지니는 경우가 많음
  - 하지만 주로 이미지나 텍스트 데이터는 *저수준 대표성*을 지님
- 은닉층은 저수준의 대표성을 가진 특성변수를 고수준 대표성을 가진 특성변수로 만드는 역할을 함
  - 입력층에 입력된 특성변수는 여러 개의 은닉층을 통해 고수준의 대표성을 가진 특성변수로 변경된 후 출력층에 전달됨
- 많을 수록 좋은가?
  - 고수준의 대표성을 갖는 특성변수가 학습데이터를 지나치게 대표하여 발생하는 과대적합(overfitting) 문제 발생
  - 선형 결합 모수(파라미터)의 숫자가 늘어나 모수 추정에 실패, 딥러닝모형 자체가 무너지는 형상 발생
    - 여러 은닉층의 모수를 줄이며, 미분사라짐을 차단하는 방향으로 진행해야함

### 딥러닝의 기본 모형
- MLP(multilayer percceptron), CNN(convolutional neural networks), RNN(recurrent neural networks)등 세 가지 기본모형으로 구성
- 딥러닝은 특성변수의 추출을 모형화함
  - 딥러닝 기본모형 이해를 위해서는 추정해야 할 모수의 수를 계산할 수 있어야 함
    - 아마 시험 나올 듯

## Neural Networks
### Single Layer Neural Networks
- ![image](https://github.com/googoo9918/TIL/assets/102513932/ab59dea5-d08b-42e0-826f-a84ab866e476)
  - 예측 함수
    - Y = f(X)
      - 입력 벡터 X로부터 응답 Y를 예측함
      - 비선형 함수(Non-linear)
    - Y
      - 응답 또는 예측하고자 하는 대상
    - X
      - p개의 변수를 포함하는 입력 벡터
  - 단일층 신경망 모델(Single layer Neural Network Model)
    - f(X) = β0 + ∑ᵏ₌₁ βk hk(X) = β₀ + ∑ᵏ₌₁ βₖ g(wₖ₀ + ∑ⱼ₌₁ᵖ wₖⱼ Xⱼ) 
      - 비선형 함수 f(X)는 바이어스 항 β0, 가중치 βk, 은닉층 활성화 함수 hk를 사용하여 계산
      - β0
        - 바이어스 항(절편)
        - 활성화 함수 출력에 더해져 최종 예측에 영향을 미침
      - βk
        - 은닉 레이어 -> 출력 레이어로 가는 연결의 가중치
      - wₖ₀
        - 은닉 레이어의 각 유닛에 대한 편향(바이어스 항)
      - wₖⱼ
        - 입력 레이어 -> 은닉 레이어로 가는 연결의 가중치
      - k
        - 은닉 유닛(은닉 노드)의 수를 나타냄
      - g(z)
        - 미리 지정된 비선형 활성화 함수
          - ex) 시그모이드, ReLU
    - 각 은닉 노드마다 하나씩 총 5개의 βk 가중치, 출력 노드의 편향 β0 추가 시 총 6개의 β 가중치 존재
    - 각 연결에는 고유의 가중치 wₖⱼ 존재, 4개의 입력 노드와 5개의 은닉 노드 사이에는 총 20개의 wₖⱼ 가중치 존재
      - 은닉층의 각 노드에는 자체 편향 wₖ₀이 있으므로, 5개의 은닉 노드에 각각 하나씩 총 5개의 편향이 있음
    - 총 31개의 학습 파라미터(모수) 존재
      - 20(입력-은닉 가중치)(wₖⱼ) + 5(은닉-출력 가중치)(βk) + 5(은닉 층 편향)(wₖ₀) + 1 (출력 층 편향)(β0)
      - 그냥 ( (4+1) * 5 ) + ( (5+1) *1) 이라고 생각하는게 나을듯
### Activation Function
- ![image](https://github.com/googoo9918/TIL/assets/102513932/9726ac03-abaa-47f9-8800-9f7fa7778fce)
  - A_k = h_k(X) = g(w_k0 + ∑ᵢ₌₁ᵖ w_kj X_j)
    - 활성화 함수는 신경망에서 입력 X를 받아 은닉 노드의 출력 A_k를 생성하는데 사용됨
    - g(z)는 활성화 함수(activatin function)
    - 활성화 함수
      - 시그모이드(sigmoid)
        - 1 / (1 + e^(-z))
        - (0,1) 사이의 출력을 가짐 
      - ReLU
        - max(0, z)
      - ReLU가 시그모이드에 비해 더 효율적으로 계산될 수 있고, 더 선호되는 추세
### Activation Function and Fitting
- ![image](https://github.com/googoo9918/TIL/assets/102513932/0040b37b-175a-4a6f-bc4e-de0b14949b02)
  - 활성화 함수는 주로 은닉층에서 사용
    - **비선형성**을 도입, 모델이 단순한 선형 모델을 넘어 복잡한 데이터 구조를 학습할 수 있도록 함
  - β_1, β_2는 활성화 함수의 계수, w는 활성화 함수에 대한 가중치
    - 사진의 수식을 잘 이해해 볼 것
  - 모델의 적합성은 관측된 데이터 Y_i와 모델에 의해 예측된 f(X_i)사이의 차이를 최소화 함
    - 회귀 문제에서는 오차의 제곱합을 최소화함
    - 활성화 함수를 통한 출력값은 기존 특성의 비선형 조합, 파생된 특성과 같은 역할을 하며 모델의 표현력을 증가시킴

### Multilayer Neural Network
- 다중 신경망
  - 여러 개의 은닉층을 가짐
  - 각 은닉층에 많은 수의 유닛 존재
  - 단일 은닉층을 가진 신경망이라도, 많은 수의 유닛을 가질 경우 대부분 함수를 근사(approximate)할 수 있음
    - 다만, 다중 신경망을 사용하는 것이 더 쉬움
      - 각각의 층이 상대적으로 적당한 크기를 가진다면, 신경망은 특징의 다양한 수준을 추출하고 복잡한 함수를 더 쉽게 학습할 수 있음
- Example: MNIST Digits
  - ![image](https://github.com/googoo9918/TIL/assets/102513932/647190b6-70cc-4ffe-b751-16b16fac1c97)
  - MNIST 데이터셋은 기계 학습 분야에서 가장 기본적인 데이터셋
    - 손으로 쓴 숫자들의 큰 집합을 포함함
    - 특성
      - 28*28(=784) 픽셀의 grayscale 이미지
        - 이는 입력 벡터(X)로 사용됨 
        - 각 픽셀의 값은 0부터 255까지의 값을 가질 수 있음
      - 훈련 데이터로는 60,000(60k)이미지, 테스트 데이터로는 10,000개의 이미지
    - 라벨(출력 벡터Y)
      - 출력 벡터는 10개의 더미 변수를 가지고 있음, 이는 0부터 9까지의 숫자 클래스에 각각 대응됨
- ![image](https://github.com/googoo9918/TIL/assets/102513932/1a83a44b-cc70-4a8c-a623-2685223b8ae0)
  - 입력층은 각 픽셀 값을 나타내는 784개의 뉴런
  - 첫 번째 은닉층은 256개의 유닛으로 구성
  - 두 번째 은닉층은 128개의 유닛으로 구성
  - 출력층은 10개의 유닛
    - 각 유닛은 0부터 9까지의 MNIST 숫자 클래스 중 하나에 해당됨
- ![image](https://github.com/googoo9918/TIL/assets/102513932/3379fcbb-d831-4b48-a20d-733fa62f6afd)
  - 첫 번째 은닉층
    - 256개의 유닛
    - 가중치 행렬 W_1의 차원은 785 * 256
      - 785는 입력 레이어의 784 픽셀에 바이어스 유닛을 추가한 것
  - 두 번째 은닉층
    - 128개의 유닛
      - 가중치 행렬 W_2의 차원은 257 * 128
        - 257은 첫 번째 은닉층의 256 유닛에 바이어스 유닛을 추가한 것
  - 출력층
    - m은 0부터 9까지의 숫자 클래스 의미
    - Z_m은 다른 선형 모델을 나타냄
    - 가중치 행렬 B의 차원은 129*10임
      - 129는 두 번째 은닉층의 128 유닛에 바이어스 유닛을 추가한 것
  - 총 매개변수
    - 총 235,146의 매개변수가 있음

### Lab in Python
- 데이터 준비
```python
# 결측치 제거
.dropna()

# 훈련, 테스트 데이터 분할
train_test_split(X,Y, test_size = 1/3, random_state=1)
```
- 선형 회귀 모델
```python
# 회귀 모델 적합
.fit(X_train, Y_train)

# 예측
.predict(X_test)
```
- 라쏘 회귀 모델
```python
# 데이터 학습 + 변환 과정 결합
scaler.fit_transfrom(X_train)
# 적합
grid.fit(X_train, Y_train)
# 예측
.predict(X_test)
```
- 신경망 모델
```python
# 입력 데이터 1차원 배열로 평탄화
self.flatten()

# 신경망 계층 순차적 정의
# 입력 차원을 50개의 뉴런으로 매핑 / 비선형 활성화 함수 -> 음수 입력을 0으로 만듬 / 과적합 방지를 위해 뉴런 40%를 임의로 비활성화 / 50개 뉴런을 최종 출력인 1차원으로 매핑
self.sequential = nn.Sequential(
            nn.Linear(input_size, 50), nn.ReLU(), nn.Dropout(0.4), nn.Linear(50, 1)
        )
# Hitters의 19개 특성이 50차원으로 매핑 -> 50 * (19+1)의 파라미터가 됨
# 이후 40%의 드롭아웃 계층을 거침
# 마지막으로 1차원으로의 선형 매핑, 다시 편향이 도입됨 50 +1
# 따라서 전체 파라미터는 1000 + 50 + 1 --> 1051임
```
- PyTorch
```python
# NumPy배열에서 파이토치 텐서로 변환
torch.tensor(X_train.astype(np.float32))

# 회귀 모델 설정, 평가 지표로 평균 절대 오차 사용
hit_module = SimpleModule.regression(hit_model, metrics={'mae':MeanAbsoluteError()})

# SGD(확률적 경사 하강법)은 데이터셋의 일부만을 사용, 각 단계에서 그리디언트 계산하고 모델 매개변수를 업데이트함
# 에폭 계산, 에폭은 훈련 데이터셋을 한 번 순회하는 것을 의미함 hit_dm에서 batch_size를 32로 지정했으므로, 한 에폭은 175/32 = 5.5 SGD 단계임 --> 총 6번의 단계가 필요하게 됨
```

- 모델 평가
```python
# 모델을 평가 모드로 설정, 드롭아웃과 같은 레이어 비활성화
.eval()
```

- Multilayer Network on the MNIST Digit Data
```python
self.layer1 = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 256),
            nn.ReLU(),
            nn.Dropout(0.4))
        self.layer2 = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3))
        self._forward = nn.Sequential(
            self.layer1,
            self.layer2,
            nn.Linear(128, 10))
# 1*28*28의 이미지를 28*28 요소의 1D 벡터로 평탄화
# 평탄화된 벡터를 256차원의 히든 레이어로 선형 변환
# 이때 파라미터 --> 785 * 256 = 200, 960

# 256 -> 128
# 257 * 128 = 32,896

# 128 -> 10
# 129 * 10 = 1,290
```
- 다중 클래스 로지스틱 회귀 모델
```python
# 분류 모델 생성, (훈련 모델, 클래스 수)
SimpleModule.classification(mlr_model, num_classes=10)

# 모델 적합
.fit(mlr_module, datamoudle=mnist_dm)

# 모델 평가
.test(mlr_module, datamodule=mnist_dm)
```

## Convolutional Nerual Networks(CNN)
- CNN은 이미지 인식과 분류 작업에서 탁월한 딥러닝 모델
- 여러 계층으로 구성
- CIFAT100
  - 32*32 픽셀 크기의 자연 이미지
  - 100개의 다양한 클래스
    - 각 클래스가 5개의 하위 클래스를 갖는 20개의 슈퍼 클래스로 구성
- 핵심 컴포넌트
  - 컨볼루션(convolution) 계층
    - 입력 이미지의 작은 부분에 대해 필터를 적용
    - 이를 통해 이미지의 중요 특징 감지

### How Work
- 컨볼루션 계층
  - 이미지에서 작은 패턴이나 특징을 감지하는 역할
  - 선이나 가장자리와 같은 기본 형태 감지 가능
  - 필터(커널)이라는 작은 행렬을 이미지 전체에 걸쳐 슬라이딩
    - 특징 맵 생성
    - 이 맵은 입력 이미지에서 각 필터가 반응하는 영역의 강도를 나타냄
- 풀링 계층(Pooling layers)
  - 컨볼루션 계층 다음에는 풀링 계층 위치
  - 이미지의 공간 크기를 줄여주는 역할을 함
  - 이를 통해 계산량을 감소, 가장 두드러진 정보 유지 + 과적합 방지
- 계층적 구성
- 최종 분류

### 컨볼루션 계층(Convolution Layer)
- 컨볼루션 계층은 다수의 컨볼루션 필터로 구성
- 컨볼루션 필터는 컨볼루션이라고 불리는 간단한 연산에 의지함
- ![image](https://github.com/googoo9918/TIL/assets/102513932/ffa0d075-b391-4597-93fc-d7a0afc9a043)
  - 만약 원본 이미지의 2*2 부분 행렬이 컨볼루션 필터와 유사하다면, 변환된 이미지에서 큰 값이 나타나고 아니라면 작은 값이 나타남
    - 입력 이미지의 부분 이미지가 필터와 비슷하면 점수가 높고, 그렇지 않으면 점수가 낮음
  - 필터 자체도 하나의 이미지, 작은 형태나 가장자리 등을 나타냄
    - 이 필터를 입력 이미지 위에서 이동시키면서 일치하는 부분에 대한 점수를 매김
    - 점수 매기기는 내적을 사용하여 수행됨
- 컨불루션 필터는 이미지의 다양한 부분에서 발생하는 공통 패턴을 찾기 위함임
  - 컨볼루션의 결과는 새로운 특징 맵(feature map)임

### Pooling Layser
- Pooling Layer는 큰 이미지를 작은 요약 이미지로 압축하는 츠
- Max Pooling
  - 2*2 블록의 네 개 픽셀 중 최대값을 선택, 해당 블록을 대체
  - 이미지의 크기를 각 방향으로 절반으로 줄임
  - 위치 불변성(location invariance)를 제공함
    - 하나의 픽셀이 큰 값을 갖는 경우, 블록 전체가 축소된 이미지에서도 큰 값으로 등록 -> 불변
    - 중요 특징을 유지하면서도 데이터의 양을 효과적으로 줄일 수 있음

### Architecture of CNN
- CNN은 다수의 컨볼루션 층과 풀링 층으로 구성됨
- ex) 3개의 컨볼루션 layer와 Pooling Layer
- 필터는 일반적으로 작은 크기(2*2)를 가짐, 각 필터는 컨볼루션 층에서 새로운 채널을 생성함
- 풀링 층은 이