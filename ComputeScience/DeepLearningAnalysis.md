# 딥러닝 분석
## 딥러닝
### 딥러닝
- ![image](https://github.com/googoo9918/TIL/assets/102513932/ec382bb6-d69c-4687-af56-b5180d566c26)
  - 딥러닝과 머신러닝
    - 공통점
      - 손실함수를 최소화
    - 차이점
      - 딥러닝은 머신러닝에 비해 인간의 인지능력에 강점
      - 은닉층 존재 여부
- ![image](https://github.com/googoo9918/TIL/assets/102513932/3ff859fb-0629-4ca2-8afd-b4875a92f1d4)
  - 독립변수(feature; x)들의 선형 및 비선형 결합을 통해 목적변수(종속변수, output; y)를 확률적으로 추정
  - 딥러닝은 은닉층 존재, 머신러닝은 은닉층 업음
### 은닉층
- *특성변수*는 회귀와 분류를 위한 *목적변수*를 예측하는데 이용
  - 특성변수는 *고수준의 대표성*을 지니는 경우가 많음
  - 하지만 주로 이미지나 텍스트 데이터는 *저수준 대표성*을 지님
- 은닉층은 저수준의 대표성을 가진 특성변수를 고수준 대표성을 가진 특성변수로 만드는 역할을 함
  - 입력층에 입력된 특성변수는 여러 개의 은닉층을 통해 고수준의 대표성을 가진 특성변수로 변경된 후 출력층에 전달됨
- 많을 수록 좋은가?
  - 고수준의 대표성을 갖는 특성변수가 학습데이터를 지나치게 대표하여 발생하는 과대적합(overfitting) 문제 발생
  - 선형 결합 모수(파라미터)의 숫자가 늘어나 모수 추정에 실패, 딥러닝모형 자체가 무너지는 형상 발생
    - 여러 은닉층의 모수를 줄이며, 미분사라짐을 차단하는 방향으로 진행해야함

### 딥러닝의 기본 모형
- MLP(multilayer percceptron), CNN(convolutional neural networks), RNN(recurrent neural networks)등 세 가지 기본모형으로 구성
- 딥러닝은 특성변수의 추출을 모형화함
  - 딥러닝 기본모형 이해를 위해서는 추정해야 할 모수의 수를 계산할 수 있어야 함
    - 아마 시험 나올 듯

## Neural Networks
### Single Layer Neural Networks
- ![image](https://github.com/googoo9918/TIL/assets/102513932/ab59dea5-d08b-42e0-826f-a84ab866e476)
  - 예측 함수
    - Y = f(X)
      - 입력 벡터 X로부터 응답 Y를 예측함
      - 비선형 함수(Non-linear)
    - Y
      - 응답 또는 예측하고자 하는 대상
    - X
      - p개의 변수를 포함하는 입력 벡터
  - 단일층 신경망 모델(Single layer Neural Network Model)
    - f(X) = β0 + ∑ᵏ₌₁ βk hk(X) = β₀ + ∑ᵏ₌₁ βₖ g(wₖ₀ + ∑ⱼ₌₁ᵖ wₖⱼ Xⱼ) 
      - 비선형 함수 f(X)는 바이어스 항 β0, 가중치 βk, 은닉층 활성화 함수 hk를 사용하여 계산
      - β0
        - 바이어스 항(절편)
        - 활성화 함수 출력에 더해져 최종 예측에 영향을 미침
      - βk
        - 은닉 레이어 -> 출력 레이어로 가는 연결의 가중치
      - wₖ₀
        - 은닉 레이어의 각 유닛에 대한 편향(바이어스 항)
      - wₖⱼ
        - 입력 레이어 -> 은닉 레이어로 가는 연결의 가중치
      - k
        - 은닉 유닛(은닉 노드)의 수를 나타냄
      - g(z)
        - 미리 지정된 비선형 활성화 함수
          - ex) 시그모이드, ReLU
    - 각 은닉 노드마다 하나씩 총 5개의 βk 가중치, 출력 노드의 편향 β0 추가 시 총 6개의 β 가중치 존재
    - 각 연결에는 고유의 가중치 wₖⱼ 존재, 4개의 입력 노드와 5개의 은닉 노드 사이에는 총 20개의 wₖⱼ 가중치 존재
      - 은닉층의 각 노드에는 자체 편향 wₖ₀이 있으므로, 5개의 은닉 노드에 각각 하나씩 총 5개의 편향이 있음
    - 총 31개의 학습 파라미터(모수) 존재
      - 20(입력-은닉 가중치)(wₖⱼ) + 5(은닉-출력 가중치)(βk) + 5(은닉 층 편향)(wₖ₀) + 1 (출력 층 편향)(β0)
      - 그냥 ( (4+1) * 5 ) + ( (5+1) *1) 이라고 생각하는게 나을듯
### Activation FUnction
- ![image](https://github.com/googoo9918/TIL/assets/102513932/9726ac03-abaa-47f9-8800-9f7fa7778fce)
  - A_k = h_k(X) = g(w_k0 + ∑ᵢ₌₁ᵖ w_kj X_j)
    - 활성화 함수는 신경망에서 입력 X를 받아 은닉 노드의 출력 A_k를 생성하는데 사용됨
    - g(z)는 활성화 함수(activatin function)
    - 활성화 함수
      - 시그모이드(sigmoid)
        - 1 / (1 + e^(-z))
        - (0,1) 사이의 출력을 가짐 
      - ReLU
        - max(0, z)
      - ReLU가 시그모이드에 비해 더 효율적으로 계산될 수 있고, 더 선호되는 추세
### Activation Function and Fitting
- ![image](https://github.com/googoo9918/TIL/assets/102513932/0040b37b-175a-4a6f-bc4e-de0b14949b02)
  - 활성화 함수는 주로 은닉층에서 사용
    - **비선형성**을 도입, 모델이 단순한 선형 모델을 넘어 복잡한 데이터 구조를 학습할 수 있도록 함
  - β_1, β_2는 활성화 함수의 계수, w는 활성화 함수에 대한 가중치
    - 사진의 수식을 잘 이해해 볼 것
  - 모델의 적합성은 관측된 데이터 Y_i와 모델에 의해 예측된 f(X_i)사이의 차이를 최소화 함
    - 회귀 문제에서는 오차의 제곱합을 최소화함
    - 활성화 함수를 통한 출력값은 기존 특성의 비선형 조합, 파생된 특성과 같은 역할을 하며 모델의 표현력을 증가시킴

### Multilayer Neural Network
- 다중 신경망
  - 여러 개의 은닉층을 가짐
  - 각 은닉층에 많은 수의 유닛 존재
  - 단일 은닉층을 가진 신경망이라도, 많은 수의 유닛을 가질 경우 대부분 함수를 근사(approximate)할 수 있음
    - 다만, 다중 신경망을 사용하는 것이 더 쉬움
      - 각각의 층이 상대적으로 적당한 크기를 가진다면, 신경망은 특징의 다양한 수준을 추출하고 복잡한 함수를 더 쉽게 학습할 수 있음
- Example: MNIST Digits
  - ![image](https://github.com/googoo9918/TIL/assets/102513932/647190b6-70cc-4ffe-b751-16b16fac1c97)
  - MNIST 데이터셋은 기계 학습 분야에서 가장 기본적인 데이터셋
    - 손으로 쓴 숫자들의 큰 집합을 포함함
    - 특성
      - 28*28(=784) 픽셀의 grayscale 이미지
        - 이는 입력 벡터(X)로 사용됨 
        - 각 픽셀의 값은 0부터 255까지의 값을 가질 수 있음
      - 훈련 데이터로는 60,000(60k)이미지, 테스트 데이터로는 10,000개의 이미지
    - 라벨(출력 벡터Y)
      - 출력 벡터는 10개의 더미 변수를 가지고 있음, 이는 0부터 9까지의 숫자 클래스에 각각 대응됨
- ![image](https://github.com/googoo9918/TIL/assets/102513932/1a83a44b-cc70-4a8c-a623-2685223b8ae0)
  - 입력층은 각 픽셀 값을 나타내는 784개의 뉴런
  - 첫 번째 은닉층은 256개의 유닛으로 구성
  - 두 번째 은닉층은 128개의 유닛으로 구성
  - 출력층은 10개의 유닛
    - 각 유닛은 0부터 9까지의 MNIST 숫자 클래스 중 하나에 해당됨
- ![image](https://github.com/googoo9918/TIL/assets/102513932/3379fcbb-d831-4b48-a20d-733fa62f6afd)
  - 첫 번째 은닉층
    - 256개의 유닛
    - 가중치 행렬 W_1의 차원은 785 * 256
      - 785는 입력 레이어의 784 픽셀에 바이어스 유닛을 추가한 것
  - 두 번째 은닉층
    - 128개의 유닛
      - 가중치 행렬 W_2의 차원은 257 * 128
        - 257은 첫 번째 은닉층의 256 유닛에 바이어스 유닛을 추가한 것
  - 출력층
    - m은 0부터 9까지의 숫자 클래스 의미
    - Z_m은 다른 선형 모델을 나타냄
    - 가중치 행렬 B의 차원은 129*10임
      - 129는 두 번째 은닉층의 128 유닛에 바이어스 유닛을 추가한 것
  - 총 매개변수
    - 총 235,146의 매개변수가 있음